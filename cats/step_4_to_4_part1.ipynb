{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Training Set\n",
    "\n",
    "**Author(s):** kozyr@google.com, bfoo@google.com\n",
    "\n",
    "In this notebook, we gather exploratory data from our training set to do feature engineering and model tuning. Before running this notebook, make sure that:\n",
    "\n",
    "* You have already run steps 2 and 3 to collect and split your data into training, validation, and test. \n",
    "* Your training data is in a Google storage folder such as gs://[your-bucket]/[dataprep-dir]/training_images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spirit of learning to walk before learning to run, we'll write this notebook in a more basic style than you'll see in a professional setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "**TODO for you:** In Screen terminal 1 (to begin with Screen in the VM, first type\n",
    "`screen` and `Ctrl+a c`), go to the VM shell and type `Ctrl+a 1`,\n",
    "create a folder to store your training and debugging images, and then copy a small\n",
    "sample of training images from Cloud Storage:\n",
    "```\n",
    "mkdir -p ~/data/training_small\n",
    "gsutil -m cp gs://$BUCKET/catimages/training_images/000*.png ~/data/training_small/\n",
    "gsutil -m cp gs://$BUCKET/catimages/training_images/001*.png ~/data/training_small/\n",
    "mkdir -p ~/data/debugging_small\n",
    "gsutil -m cp gs://$BUCKET/catimages/training_images/002*.png ~/data/debugging_small\n",
    "echo \"done!\"\n",
    "```\n",
    "\n",
    "Note that we only take the images starting with those IDs to limit the total number we'll copy over to under 3 thousand images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your username:\n",
    "YOUR_GMAIL_ACCOUNT = '******' # Whatever is before @gmail.com in your email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for this section:\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the filenames:\n",
    "TRAINING_DIR = os.path.join('/home', YOUR_GMAIL_ACCOUNT, 'data/training_small/')\n",
    "files = os.listdir(TRAINING_DIR)  # Grab all the files in the VM images directory\n",
    "print(files[0:5])  # Let's see some filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eyes on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pictures(filelist, dir, img_rows=2, img_cols=3, figsize=(20, 10)):\n",
    "  \"\"\"Display the first few images.\n",
    "\n",
    "  Args:\n",
    "    filelist: list of filenames to pull from\n",
    "    dir: directory where the files are stored\n",
    "    img_rows: number of rows of images to display\n",
    "    img_cols: number of columns of images to display\n",
    "    figsize: sizing for inline plots\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  plt.close('all')\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "\n",
    "  for i in range(img_rows * img_cols):\n",
    "    a=fig.add_subplot(img_rows, img_cols,i+1)\n",
    "    img = mpimg.imread(os.path.join(dir, filelist[i]))\n",
    "    plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pictures(files, TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the colors at [rapidtables.com/web/color/RGB_Color](http://www.rapidtables.com/web/color/RGB_Color.htm), but don't forget to flip order of the channels to BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the actual image matrix look like?  There are three channels:\n",
    "img = cv2.imread(os.path.join(TRAINING_DIR, files[0]))\n",
    "print('\\n***Colors in the middle of the first image***\\n')\n",
    "print('Blue channel:')\n",
    "print(img[63:67,63:67,0])\n",
    "print('Green channel:')\n",
    "print(img[63:67,63:67,1])\n",
    "print('Red channel:')\n",
    "print(img[63:67,63:67,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bgr(filelist, dir, img_rows=2, img_cols=3, figsize=(20, 10)):\n",
    "  \"\"\"Make histograms of the pixel color matrices of first few images.\n",
    "\n",
    "  Args:\n",
    "    filelist: list of filenames to pull from\n",
    "    dir: directory where the files are stored\n",
    "    img_rows: number of rows of images to display\n",
    "    img_cols: number of columns of images to display\n",
    "    figsize: sizing for inline plots\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  plt.close('all')\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  color = ('b','g','r')\n",
    "\n",
    "  for i in range(img_rows * img_cols):\n",
    "    a=fig.add_subplot(img_rows, img_cols, i + 1)\n",
    "    img = cv2.imread(os.path.join(TRAINING_DIR, files[i]))\n",
    "    for c,col in enumerate(color):\n",
    "      histr = cv2.calcHist([img],[c],None,[256],[0,256])\n",
    "      plt.plot(histr,color = col)\n",
    "      plt.xlim([0,256])\n",
    "      plt.ylim([0,500])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bgr(files, TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some sanity checks\n",
    "\n",
    "For example:\n",
    "* Do we have blank images?\n",
    "* Do we have images with very few colors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in blue channel for each image, reshape to vector, count unique values:\n",
    "unique_colors = []\n",
    "landscape = []\n",
    "for f in files:\n",
    "  img = np.array(cv2.imread(os.path.join(TRAINING_DIR, f)))[:,:,0]\n",
    "  # Determine if landscape is more likely than portrait by comparing\n",
    "    #amount of zero channel in 3rd row vs 3rd col:\n",
    "  landscape_likely = (np.count_nonzero(img[:,2]) > np.count_nonzero(img[2,:])) * 1\n",
    "  # Count number of unique blue values:\n",
    "  col_count = len(set(img.ravel()))\n",
    "  # Append to array:\n",
    "  unique_colors.append(col_count)\n",
    "  landscape.append(landscape_likely)\n",
    "    \n",
    "unique_colors = pd.DataFrame({'files': files, 'unique_colors': unique_colors,\n",
    "                              'landscape': landscape})\n",
    "unique_colors = unique_colors.sort_values(by=['unique_colors'])\n",
    "print(unique_colors[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pictures with the lowest diversity of unique color values:\n",
    "suspicious = unique_colors['files'].tolist()\n",
    "show_pictures(suspicious, TRAINING_DIR, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get labels\n",
    "\n",
    "Extract labels from the filename and create a pretty dataframe for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(str):\n",
    "  \"\"\"\n",
    "  Split out the label from the filename of the image, where we stored it.\n",
    "  Args:\n",
    "    str: filename string.\n",
    "  Returns:\n",
    "    label: an integer 1 or 0\n",
    "  \"\"\"\n",
    "  split_filename = str.split('_')\n",
    "  label = int(split_filename[-1].split('.')[0])\n",
    "  return(label)\n",
    "\n",
    "# Example:\n",
    "get_label('12550_0.1574_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unique_colors[:]\n",
    "df['label'] = df['files'].apply(lambda x: get_label(x))\n",
    "df['landscape_likely'] = df['landscape']\n",
    "df = df.drop(['landscape', 'unique_colors'], axis=1)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feature Engineering\n",
    "\n",
    "Below, we show an example of a very simple set of features that can be derived from an image. This function simply pulls the mean, standard deviation, min, and max of pixel values in one image band (red, green, or blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_img_features(band):\n",
    "  \"\"\"\n",
    "  Define a set of features that we can look at for each color band\n",
    "  Args:\n",
    "    band: array which is one of blue, green, or red\n",
    "  Returns:\n",
    "    features: unique colors, nonzero count, mean, standard deviation,\n",
    "              min, and max of the channel's pixel values\n",
    "  \"\"\"\n",
    "  return [len(set(band.ravel())), np.count_nonzero(band),\n",
    "          np.mean(band), np.std(band),\n",
    "          band.min(), band.max()]\n",
    "\n",
    "def concat_all_band_features(file, dir):\n",
    "  \"\"\"\n",
    "  Extract features from a single image.\n",
    "   Args:\n",
    "         file - single image filename\n",
    "         dir - directory where the files are stored\n",
    "  Returns:\n",
    "         features - descriptive statistics for pixels\n",
    "  \"\"\"\n",
    "  img = cv2.imread(os.path.join(dir, file))\n",
    "  features = []\n",
    "  blue = np.float32(img[:,:,0])\n",
    "  green = np.float32(img[:,:,1])\n",
    "  red = np.float32(img[:,:,2])\n",
    "  features.extend(general_img_features(blue)) # indices 0-4\n",
    "  features.extend(general_img_features(green)) # indices 5-9\n",
    "  features.extend(general_img_features(red)) # indices 10-14\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see an example:\n",
    "print(files[0] + '\\n')\n",
    "example = concat_all_band_features(files[0], TRAINING_DIR)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply it to our dataframe:\n",
    "feature_names = ['blue_unique', 'blue_nonzero', 'blue_mean', 'blue_sd', 'blue_min', 'blue_max',\n",
    "                 'green_unique', 'green_nonzero', 'green_mean', 'green_sd', 'green_min', 'green_max',\n",
    "                 'red_unique', 'red_nonzero', 'red_mean', 'red_sd', 'red_min', 'red_max']\n",
    "\n",
    "# Compute a series holding all band features as lists\n",
    "band_features_series = df['files'].apply(lambda x: concat_all_band_features(x, TRAINING_DIR))\n",
    "\n",
    "# Loop through lists and distribute them across new columns in the dataframe\n",
    "for i in range(len(feature_names)):\n",
    "  df[feature_names[i]] = band_features_series.apply(lambda x: x[i])\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are these features good for finding cats?\n",
    "# Let's look at some basic correlations.\n",
    "df.corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coarse features look pretty bad individually. Most of this is due to features capturing absolute pixel values. But photo lighting could vary significantly between different image shots. What we end up with is a lot of noise.\n",
    "\n",
    "Are there some better feature detectors we can consider? Why yes, there are! Several common features involve finding corners in pictures, and looking for pixel gradients (differences in pixel values between neighboring pixels in different directions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris Corner Detector\n",
    "\n",
    "The following snippet runs code to visualize harris corner detection for a few sample images. Configuring the threshold determines how strong of a signal we need to determine if a pixel corresponds to a corner (high pixel gradients in all directions).\n",
    "\n",
    "Note that because a Harris corner detector returns another image map with values corresponding to the likelihood of a corner at that pixel, it can also be fed into general_img_features() to extract additional features. What do you notice about corners on cat images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.05\n",
    "\n",
    "def show_harris(filelist, dir, band=0, img_rows=4, img_cols=4, figsize=(20, 10)):\n",
    "  \"\"\"\n",
    "  Display Harris corner detection for the first few images.\n",
    "  Args:\n",
    "    filelist: list of filenames to pull from\n",
    "    dir: directory where the files are stored\n",
    "    band: 0 = 'blue', 1 = 'green', 2 = 'red'\n",
    "    img_rows: number of rows of images to display\n",
    "    img_cols: number of columns of images to display\n",
    "    figsize: sizing for inline plots\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  plt.close('all')\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "\n",
    "  def plot_bands(src, band_img):\n",
    "    a=fig.add_subplot(img_rows, img_cols, i + 1)\n",
    "    dst = cv2.cornerHarris(band_img, 2, 3, 0.04)\n",
    "    dst = cv2.dilate(dst,None) # dilation makes the marks a little bigger\n",
    "\n",
    "    # Threshold for an optimal value, it may vary depending on the image.\n",
    "    new_img = src.copy()\n",
    "    new_img[dst > THRESHOLD * dst.max()]=[0, 0, 255]\n",
    "    # Note: openCV reverses the red-green-blue channels compared to matplotlib,\n",
    "    # so we have to flip the image before showing it\n",
    "    imgplot = plt.imshow(cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "  for i in range(img_rows * img_cols):\n",
    "    img = cv2.imread(os.path.join(dir, filelist[i]))\n",
    "    plot_bands(img, img[:,:,band])\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_harris(files, TRAINING_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
